%!TEX program = xelatex

\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[final]{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage[UTF8]{ctex}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{multirow}
\newtheorem{definition}{定义}
\newtheorem{theorem}{定理}
\usepackage{subcaption}
\usepackage[export]{adjustbox}

\setCJKmainfont{SimSun}[AutoFakeBold=2.5,ItalicFont=KaiTi]%
\setCJKsansfont{SimHei}[AutoFakeBold=2.5]%
\setCJKmonofont{FangSong}%


\title{《最优化方法》上机作业1：线搜索、牛顿法与拟牛顿法数值实验}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  张旻昊 Minhao Zhang 2101213233 \\
  前沿交叉学科研究院 Academy for Advanced Interdisciplinary Studies\\
  北京大学 Peking University\\
  颐和园路5号，海淀，北京 Yiheyuan Rd. $5^{th}$, Haidian, Beijing\\
  \texttt{minhaozhang@pku.edu.cn} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}
%\begin{CJK*}{UTF8}{gbsn}

\maketitle

\begin{abstract}
  本实验实现了线搜索、Newton方法及拟Newton方法的程序（各包括多种算法），并利用Ackley函数测试各种牛顿型方法的收敛性能、精度等数值表现，此外还进一步分析了线搜索及初始值对收敛过程的影响。
  
\end{abstract}

\section{实验设定}
\subsection{目标函数选取}
\begin{figure}[h]
  \centering
  \includegraphics[width=.65\linewidth]{pics/ackley.png}
  \caption{二维Ackley函数的可视化}
  \label{fig:ackley}
\end{figure}

本实验通过Ackley函数评估不同方法，其形式如下：
\[ \texttt{Ackley}(x) = -20 exp[-\frac{1}{5} \sqrt{\frac{1}{n}\sum\limits_{i=1}^n x_i^2}] - exp[\frac{1}{n} \sum\limits_{i=1}^n cos(2\pi x_i)] + 20+e \]

Ackley函数具有全局最小值$x^* = (0,0,...,0), f(x^*)=0$，此外它还有众多极小值，其二维形式如图\ref{fig:ackley}所示，我们的算法旨在找到其中任一个局部极小值点。

\subsection{线搜索算法}
本实验实现了两类线搜索算法：
\begin{itemize}
  \item \textbf{精确线搜索}。采用Fibonacci法进行精确线搜索，算法过程如图\ref{fig:fib_algo}所示。在实验中，首先利用进退法获取初始搜索区间；此后进行Fibonacci搜索，给定函数调用次数n，利用此方法迭代n-1次即可获取精确搜索区间，我们取此区间的中点作为搜索结果$\alpha$。默认情况下，本文的实验设置$n=20$，进退法的初始步长$\alpha_0=2$，步长变化量$step=1$，步长扩大因子$mag=2$。
  \begin{figure}[h]
    \centering
    \includegraphics[width=.8\linewidth]{pics/fib_algo.jpg}
    \caption{Fibonacci搜索算法}
    \label{fig:fib_algo}
  \end{figure}
  \item \textbf{非精确线搜索}。使用非单调GLL线搜索进行非精确线搜索；GLL准则类似于Armijo准则，但对于下降方向$d_k$，它并不要求本步所取的$\alpha$让函数值相比上一步总是下降，相反它仅要求相比前几步中的某一步下降即可，这加快了搜索速度进而提高了线搜索效率，因此它也是一种非单调线搜索准则。
  
  具体地，基于GLL准则的线搜索算法从$\alpha_0 = 1$开始，不断搜索满足如下条件的$alpha$；如果$\alpha_k$不满足，更新$\alpha_{k+1} = \sigma\alpha_k$继续判断：
  \[ f(x_k + \alpha d_k) \leq \min\limits_{0\leq j \leq min(k, M)} f(x_{k-j}) + \rho g_k^T d_k \alpha \]
  可见该算法设置一个滑动窗口M，新函数值只要相对过去的窗口内的某个函数值满足Armijo准则即可。在实验中，设置$\sigma=0.5, \rho=1e-3, M=5$。
\end{itemize}


\subsection{Newton方法}
本实验实现了两种Newton方法：
\begin{itemize}
  \item \textbf{阻尼Newton法}。沿用教材中的算法，利用$-g_k = G_k d_k$求搜索方向，再通过线搜索（默认情况下用上述精确线搜索算法）得到步长$\alpha_k$以通过$x_{k+1} = x_k + \alpha_k d_k$更新。
  \item \textbf{Cholesky修正Newton法}。上面的方法主要问题在于迭代时$G_k$常常不正定，造成$d_k$并非下降方向，为此通常利用$B_k = G_k + \nu I$修正Hessian矩阵使其正定再求解Newton方程。然而，$\nu$的取值并不容易快速确定，这里的Cholesky修正就是一种典型的高效修正方法。具体地，它设置初始$\nu = max(0, \min\limits_i G_k^{(i,i)})$，并对$B_k = G_k + \nu I$进行Cholesky分解，若成功则近似$B_k$已经“正定”，把它当作$G_k$的修正矩阵带入Newton方程；否则以指数形式增长$\nu = \sigma \nu$，继续迭代直至Cholesky分解成功或达到最大循环次数。在实验中，设置$\sigma=5$，最大循环次数为20。
\end{itemize}


\subsection{拟Newton方法}
本实验实现了三种经典拟Newton方法：SR1，DFP和BFGS，使用$H_k$的更新公式以避免额外的矩阵求逆开销，公式的形式与教材中完全相同，这里不再赘述。

\section{基本实验结果}
本节比较上述2种Newton方法及3种拟Newton方法在Ackley函数上的收敛情况并进行分析。对于所有算法，通过标准正态分布随机选取长度为$n\in \{ 8, 16, 32, 64, 128 \}$的初始向量$x_0$，所有Newton型方法迭代最多100次，当满足$||g(x_k)|| < \epsilon \wedge |f(x_{k}) - f(x_{k-1})| < \epsilon, \epsilon=1e-8$时可提前终止迭代。所得结果如表\ref{tab:overall}所示。根据表中结果我们做出如下几点分析与讨论：

\begin{table*}[h]
  \centering
  \begin{tabular}{c l c c c c c c c}
    \toprule
    \bfseries n & \bfseries Methods & \boldmath{$f(x^*)$} & \boldmath $||g(x^*)||$ & \bfseries Time(s) & \bfseries niter & \bfseries feval & \bfseries geval & \bfseries Geval \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{8} &
    DN & 6.3400 & 1.72e-6 & 0.9913 & 100 & 2250 & 100 & 100 \\
    & Cholesky  & 6.0174 & $<\epsilon$ & 0.0967 & 8 & 221 & 9 & 8 \\
    & SR1 & 2.5799 & $<\epsilon$ & 0.3142 & 40 & 1127 & 41 & 0 \\
    & DFP & 5.7135 & $<\epsilon$ & 0.1194 & 17 & 475 & 18 & 0 \\
    & BFGS & 1.2967 & $<\epsilon$ & 0.3226 & 44 & 1243 & 45 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{16} &
    DN & 8.2490 & 6.6e-7 & 1.3399 & 100 & 2576 & 100 & 100 \\
    & Cholesky  & 7.4247 & $<\epsilon$ & 0.0789 & 6 & 177 & 7 & 6 \\
    & SR1 & 7.1448 & $<\epsilon$ & 0.1835 & 31 & 881 & 32 & 0 \\
    & DFP & 8.1069 & $<\epsilon$ & 0.2173 & 35 & 995 & 36 & 0 \\
    & BFGS & 7.4247 & $<\epsilon$ & 0.1423 & 24 & 665 & 25 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{32} &
    DN & 5.1104 & 2.4e-7 & 1.9412 & 100 & 2640 & 100 & 100 \\
    & Cholesky & 6.1600 & $<\epsilon$ & 0.1519 & 8 & 231 & 9 & 8 \\
    & SR1 & 6.1600 & $<\epsilon$ & 0.1147 & 19 & 521 & 20 & 0 \\
    & DFP & 2.1522 & $<\epsilon$ & 0.2753 & 49 & 1311 & 50 & 0 \\
    & BFGS & 4.7119 & $<\epsilon$ & 0.1764 & 30 & 823 & 31 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{64} &
    DN & 8.0151 & 1.23e-5 & 3.2566 & 100 & 2524 & 100 & 100 \\
    & Cholesky & 6.7745 & $<\epsilon$ & 0.2441 & 8 & 229 & 9 & 8 \\
    & SR1 & 6.5910 & $<\epsilon$ & 0.0888 & 15 & 417 & 16 & 0 \\
    & DFP & 6.5910 & $<\epsilon$ & 0.1147 & 20 & 533 & 21 & 0 \\
    & BFGS & 6.5910 & $<\epsilon$ & 0.0977 & 15 & 411 & 16 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{128} &
    DN & 6.8139 & 7e-8 & 5.9922 & 100 & 2630 & 100 & 100 \\
    & Cholesky & 5.8812 & $<\epsilon$ & 0.5655 & 10 & 285 & 11 & 10 \\
    & SR1 & 4.5954 & $<\epsilon$ & 0.2074 & 30 & 839 & 31 & 0 \\
    & DFP & 5.7724 & $<\epsilon$ & 0.1311 & 20 & 551 & 21 & 0 \\
    & BFGS & 5.7724 & $<\epsilon$ & 0.1326 & 19 & 525 & 20 & 0 \\
    \bottomrule
  \end{tabular}
  \caption{Newton型方法在Ackley函数上的比较。所有算法均使用Fibonacci法进行精确线搜索。表中DN指阻尼牛顿法（Damped Newton），Cholesky指基于Cholesky分解的修正Newton方法；Time指CPU时间，niter为迭代轮数，feval、geval和Geval分别指Ackley函数、一阶导数及hessian矩阵的计算次数。$\epsilon=1e-8$。}
  \label{tab:overall}
\end{table*}

\begin{itemize}
  \item 阻尼Newton法收敛精度较差，基本无法达到$\epsilon=1e-8$的精度并在完成100次迭代后终止，其余方法均可实现收敛。进一步观察发现，阻尼Newton法所得$||g^*||$虽大于$\epsilon$但也较小，$f^*$也可较好地接近局部极小值，这说明该方法的正确性；如果继续迭代，发现阻尼Newton法会继续“抖动”，$||g^*||$始终无法低于$\epsilon$。为分析其原因，我记录了阻尼Newton的$d_k$并发现其常常并非下降方向，这是因为$G_k$有时并非正定，而遇到上升方向时x就会经历错误的更新进而导致“抖动”。这说明了修正$G_k$的重要性。
  \item Cholesky修正Newton方法通过尝试正定化$G_k$而具有较好的收敛性，相比所有拟Newton方法迭代次数均显著减少，这说明了精确计算$G_k$确实能带来更快的收敛，这展现了Newton方法二次收敛性的意义。
  \item 随着n的增大，虽然Newton方法始终比拟Newton方法迭代次数少，但其CPU时间却逐渐落后于拟Newton方法，这是因为计算Hessian矩阵需要更长的时间（$O(n^2)$），只需计算f和g（$O(n)$）的拟Newton方法在大规模问题上有优势，这说明拟Newton方法对于问题规模有更好地伸缩性。注意，本实验设$g(x)$和$G(x)$都作为函数已知，对于一般数值计算情况，我们需要多个$f(x)$值对这二者进行数值近似，在这种情况下Newton方法相比拟Newton方法也会有更多的函数调用次数。
  \item 拟Newton方法收敛到的函数值普遍小于Newton方法，由于Ackley函数有众多局部极小值，这并不能反映两类方法的收敛效果差别，但我推测这可能是因为拟Newton方法在迭代初期偏向负梯度方向，这使其更快的向$\mathbf{0}$更新，而Newton方法较快地趋向了最近的局部极小值，其函数值相比接近零向量的极小值更大。因此，在Ackley函数这一示例中，拟Newton方法似乎能取得比Newton方法更优的最小化结果，但这一结论并不普适：对于约靠近零向量局部极小值越大的函数，或许我们观察到的结论就会相反。
  \item 在拟Newton方法中，BFGS通常比DFP有更优的数值表现，多次实验也发现BFGS比DFP的收敛更稳定；由于更简单的形式，SR1具有较快的执行速度。
\end{itemize}


\section{进一步分析}
\subsection{不同线搜索算法比较}
本节比较前文所述的两种线搜索算法，表\ref{tab:line_search}展示了GLL和Fibonacci两种线搜索在BFGS上的实验效果（其他设定与前文相同）。

\begin{table*}[h]
  \centering
  \begin{tabular}{c l c c c c c c c}
    \toprule
    \bfseries n & \bfseries Methods & \boldmath{$f(x^*)$} & \boldmath $||g(x^*)||$ & \bfseries Time(s) & \bfseries niter & \bfseries feval & \bfseries geval & \bfseries Geval \\
    \cmidrule(lr){1-9}
    \multirow{2}{*}{8} &
    Fibonacci & 4.8841 & $<\epsilon$ & 0.2234 & 25 & 701 & 26 & 0 \\
    & GLL  & 5.8176 & $<\epsilon$ & 0.0499 & 37 & 89 & 38 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{2}{*}{16} &
    Fibonacci & 4.5308 & $<\epsilon$ & 0.2781 & 37 & 1015 & 38 & 0 \\
    & GLL  & 2.5799 & $<\epsilon$ & 0.0838 & 68 & 160 & 69 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{2}{*}{32} &
    Fibonacci & 4.6046 & $<\epsilon$ & 0.1855 & 32 & 881 & 33 & 0 \\
    & GLL  & 2.7287 & $<\epsilon$ & 0.0668 & 70 & 150 & 71 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{2}{*}{64} &
    Fibonacci & 6.0415 & $<\epsilon$ & 0.1097 & 19 & 523 & 20 & 0 \\
    & GLL  & 5.4676 & $<\epsilon$ & 0.0379 & 39 & 90 & 40 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{2}{*}{128} &
    Fibonacci & 6.8141 & $<\epsilon$ & 0.1310 & 19 & 517 & 20 & 0 \\
    & GLL  & 6.8680 & $<\epsilon$ & 0.0399 & 23 & 47 & 24 & 0 \\
    \bottomrule
  \end{tabular}
  \caption{GLL与Fibonacci两种线搜索方法在BFGS上的比较。$\epsilon=1e-8$。}
  \label{tab:line_search}
\end{table*}

可见，两算法均可实现收敛（$||g^*||<\epsilon$）；在迭代次数方面，精确线搜索的Fibonacci方法更少，这是因为精确的搜索使每步的步长选取更有效；而在CPU时间、函数调用次数等方面，非精确线搜索GLL方法显著优于精确线搜索，这是因为其每步无需全部完成Fibonacci法的20次函数调用，只需通过指数降低$\alpha$满足GLL准则，从表中可见平均每次搜索仅需1$\sim$2次函数调用，而显著减少的函数调用提高了其运行效率，让它即便需要更多的迭代轮数仍保持更低的耗时。这一结果体现了非精确线搜索的典型特征：通常迭代轮数更多，但函数调用次数显著减少，从而提升算法性能。

\subsection{初始值对收敛性的影响}
如图\ref{fig:ackley}所示，Ackley函数具有众多极小值，$\mathbf{0}$处为其全局最小值，其余的极小值离$\mathbf{0}$越远值越大。因此，初始点离$\mathbf{0}$越远往往算法就越难以收敛到更小的极小值，本节尝试验证这一点。具体地，我们在先随机初始化一个满足标准正态分布的初始向量，再将其依次乘以$k\in \{1,2,4,8,16\}$作为五个初始迭代点，分别运行基于GLL准则线搜索的Cholesky修正Newton方法，结果如表\ref{tab:init_value}所示。

可见，算法在不同初始值上均可收敛，表明其具有较好的全局收敛性。对于所有问题规模n，最终收敛到的函数值$f^*$随k的增大而增大，这符合上述结论；此外，k的变化对算法执行时间、函数调用次数等指标的影响并不显著，这说明在不同初始值处寻找局部极小值的难度差别并不十分显著。

这些结果也对我们进行数值计算实践给出了一点启发：通常而言，我们希望得到尽可能小的目标函数值，亦即寻找一个相对更优的局部极小值，在这一设定下初始值的选取变得重要，往往取得离最优局部极小值更近的初始值（例如这里的k=1）可以帮助算法收敛到更小的极值点。这一点结论与当下深度学习领域对预训练、规范初始化的相关工作之动机与结论相契合。

\begin{table*}[h]
  \centering
  \begin{tabular}{c c c c c c c c c}
    \toprule
    \bfseries n & \bfseries k & \boldmath{$f(x^*)$} & \boldmath $||g(x^*)||$ & \bfseries Time(s) & \bfseries niter & \bfseries feval & \bfseries geval & \bfseries Geval \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{8} &
       1 &  2.5799 & $<\epsilon$ & 0.0400 &  8 & 27 &  9 & 8 \\
    &  2 &  5.4958 & $<\epsilon$ & 0.0563 & 12 & 37 & 13 & 12 \\
    &  4 & 11.7645 & $<\epsilon$ & 0.0458 & 10 & 31 & 11 & 10 \\
    &  8 & 13.6390 & $<\epsilon$ & 0.0370 &  8 & 29 &  9 & 8 \\
    & 16 & 19.0864 & $<\epsilon$ & 0.0242 &  5 & 16 &  6 & 5 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{16} &
       1 &  2.5799 & $<\epsilon$ & 0.1539 & 20 & 61 & 21 & 20 \\
    &  2 &  7.6225 & $<\epsilon$ & 0.1101 & 15 & 46 & 16 & 15 \\
    &  4 & 11.9124 & $<\epsilon$ & 0.0983 &  9 & 29 & 10 & 9 \\
    &  8 & 18.0296 & $<\epsilon$ & 0.0984 & 13 & 40 & 14 & 13 \\
    & 16 & 18.2010 & $<\epsilon$ & 0.1128 & 11 & 34 & 12 & 11 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{32} &
       1 &  3.4689 & $<\epsilon$ & 0.2239 & 17 & 61 & 18 & 17 \\
    &  2 &  6.9592 & $<\epsilon$ & 0.1069 &  8 & 29 &  9 & 8 \\
    &  4 & 11.7761 & $<\epsilon$ & 0.1337 & 10 & 41 & 11 & 10 \\
    &  8 & 15.1418 & $<\epsilon$ & 0.1200 &  9 & 31 & 10 & 9 \\
    & 16 & 19.3774 & $<\epsilon$ & 0.2293 & 15 & 46 & 16 & 15 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{64} &
       1 &  3.6751 & $<\epsilon$ & 0.4887 & 20 & 68 & 21 & 20 \\
    &  2 &  7.1266 & $<\epsilon$ & 0.4013 & 16 & 49 & 17 & 16 \\
    &  4 & 12.1688 & $<\epsilon$ & 0.4218 & 17 & 86 & 18 & 17 \\
    &  8 & 16.6058 & $<\epsilon$ & 0.1751 &  7 & 24 &  8 & 7 \\
    & 16 & 19.1020 & $<\epsilon$ & 0.2447 & 10 & 34 & 11 & 10 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{128} &
       1 &  5.2257 & $<\epsilon$ & 0.6786 & 14 & 70 & 15 & 14 \\
    &  2 &  6.5065 & $<\epsilon$ & 0.3273 &  7 & 28 &  8 & 7 \\
    &  4 & 12.6906 & $<\epsilon$ & 0.3303 &  7 & 25 &  8 & 7 \\
    &  8 & 17.0561 & $<\epsilon$ & 0.3845 &  8 & 29 &  9 & 8 \\
    & 16 & 19.3248 & $<\epsilon$ & 0.8478 & 18 & 60 & 19 & 18 \\
    \bottomrule
  \end{tabular}
  \caption{不同初始值选取对Cholesky修正Newton方法的影响。其中k代表初始点相较标准正态分布的倍数，$\epsilon=1e-8$。}
  \label{tab:init_value}
\end{table*}


\section{总结}
本文比较了多种Newton和拟Newton方法在Ackley函数上的数值表现，分析了二者的优劣；本文还针对不同线搜索策略、不同初始迭代值对收敛过程的影响进行了数值实验及分析，为未来在不同问题上应用各类技巧给出了一些经验性分析。


%end{CJK*}
\end{document}
