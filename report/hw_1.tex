%!TEX program = xelatex

\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[final]{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage[UTF8]{ctex}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{multirow}
\newtheorem{definition}{定义}
\newtheorem{theorem}{定理}
\usepackage{subcaption}
\usepackage[export]{adjustbox}

\setCJKmainfont{SimSun}[AutoFakeBold=2.5,ItalicFont=KaiTi]%
\setCJKsansfont{SimHei}[AutoFakeBold=2.5]%
\setCJKmonofont{FangSong}%


\title{《最优化方法》上机作业1：线搜索、牛顿法与拟牛顿法数值实验}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  张旻昊 Minhao Zhang 2101213233 \\
  前沿交叉学科研究院 Academy for Advanced Interdisciplinary Studies\\
  北京大学 Peking University\\
  颐和园路5号，海淀，北京 Yiheyuan Rd. $5^{th}$, Haidian, Beijing\\
  \texttt{minhaozhang@pku.edu.cn} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}
%\begin{CJK*}{UTF8}{gbsn}

\maketitle

\begin{abstract}
  本实验实现了线搜索、Newton方法及拟Newton方法的程序（各包括多种算法），并利用Ackley函数测试各种牛顿型方法的收敛性能、精度等数值表现，此外还进一步分析了线搜索及初始值对收敛过程的影响。
  
\end{abstract}

\section{实验设定}
\subsection{目标函数选取}
\begin{figure}[h]
  \centering
  \includegraphics[width=.65\linewidth]{pics/ackley.png}
  \caption{二维Ackley函数的可视化}
  \label{fig:ackley}
\end{figure}

本实验通过Ackley函数评估不同方法，其形式如下：
\[ \texttt{Ackley}(x) = -20 exp[-\frac{1}{5} \sqrt{\frac{1}{n}\sum\limits_{i=1}^n x_i^2}] - exp[\frac{1}{n} \sum\limits_{i=1}^n cos(2\pi x_i)] + 20+e \]

Ackley函数具有全局最小值$x^* = (0,0,...,0), f(x^*)=0$，此外它还有众多极小值，其二维形式如图\ref{fig:ackley}所示，我们的算法旨在找到其中任一个局部极小值点。

\subsection{线搜索算法}
本实验实现了两类线搜索算法：
\begin{itemize}
  \item \textbf{精确线搜索}。采用Fibonacci法进行精确线搜索，算法过程如图\ref{fig:fib_algo}所示。在实验中，首先利用进退法获取初始搜索区间；此后进行Fibonacci搜索，给定函数调用次数n，利用此方法迭代n-1次即可获取精确搜索区间，我们取此区间的中点作为搜索结果$\alpha$。默认情况下，本文的实验设置$n=20$，进退法的初始步长$\alpha_0=2$，步长变化量$step=1$，步长扩大因子$mag=2$。
  \begin{figure}[h]
    \centering
    \includegraphics[width=.8\linewidth]{pics/fib_algo.jpg}
    \caption{Fibonacci搜索算法}
    \label{fig:fib_algo}
  \end{figure}
  \item \textbf{非精确线搜索}。使用非单调GLL线搜索进行非精确线搜索；GLL准则类似于Armijo准则，但对于下降方向$d_k$，它并不要求本步所取的$\alpha$让函数值相比上一步总是下降，相反它仅要求相比前几步中的某一步下降即可，这加快了搜索速度进而提高了线搜索效率，因此它也是一种非单调线搜索准则。
  
  具体地，基于GLL准则的线搜索算法从$\alpha_0 = 1$开始，不断搜索满足如下条件的$alpha$；如果$\alpha_k$不满足，更新$\alpha_{k+1} = \sigma\alpha_k$继续判断：
  \[ f(x_k + \alpha d_k) \leq \min\limits_{0\leq j \leq min(k, M)} f(x_{k-j}) + \rho g_k^T d_k \alpha \]
  可见该算法设置一个滑动窗口M，新函数值只要相对过去的窗口内的某个函数值满足Armijo准则即可。在实验中，设置$\sigma=0.5, \rho=1e-3, M=5$。
\end{itemize}


\subsection{Newton方法}
本实验实现了两种Newton方法：
\begin{itemize}
  \item \textbf{阻尼Newton法}。沿用教材中的算法，利用$-g_k = G_k d_k$求搜索方向，再通过线搜索（默认情况下用上述精确线搜索算法）得到步长$\alpha_k$以通过$x_{k+1} = x_k + \alpha_k d_k$更新。
  \item \textbf{Cholesky修正Newton法}。上面的方法主要问题在于迭代时$G_k$常常不正定，造成$d_k$并非下降方向，为此通常利用$B_k = G_k + \nu I$修正Hessian矩阵使其正定再求解Newton方程。然而，$\nu$的取值并不容易快速确定，这里的Cholesky修正就是一种典型的高效修正方法。具体地，它设置初始$\nu = max(0, \min\limits_i G_k^{(i,i)})$，并对$B_k = G_k + \nu I$进行Cholesky分解，若成功则近似$B_k$已经“正定”，把它当作$G_k$的修正矩阵带入Newton方程；否则以指数形式增长$\nu = \sigma \nu$，继续迭代直至Cholesky分解成功或达到最大循环次数。在实验中，设置$\sigma=5$，最大循环次数为20。
\end{itemize}


\subsection{拟Newton方法}
本实验实现了三种经典拟Newton方法：SR1，DFP和BFGS，使用$H_k$的更新公式以避免额外的矩阵求逆开销，公式的形式与教材中完全相同，这里不再赘述。

\section{基本实验结果}
本节比较上述2种Newton方法及3种拟Newton方法在Ackley函数上的收敛情况并进行分析。对于所有算法，通过标准正态分布随机选取长度为$n\in \{ 8, 16, 32, 64, 128 \}$的初始向量$x_0$，所有Newton型方法迭代最多100次，当满足$||g(x_k)|| < \epsilon \wedge |f(x_{k}) - f(x_{k-1})| < \epsilon, \epsilon=1e-8$时可提前终止迭代。所得结果如表\ref{tab:overall}所示。根据表中结果我们做出如下几点分析与讨论：

\begin{table*}[h]
  \centering
  \begin{tabular}{c l c c c c c c c}
    \toprule
    \bfseries n & \bfseries Methods & \boldmath{$f(x^*)$} & \boldmath $||g(x^*)||$ & \bfseries Time(s) & \bfseries niter & \bfseries feval & \bfseries geval & \bfseries Geval \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{8} &
    DN & 6.3400 & 1.72e-6 & 0.9913 & 100 & 2250 & 100 & 100 \\
    & Cholesky  & 6.0174 & $<\epsilon$ & 0.0967 & 8 & 221 & 9 & 8 \\
    & SR1 & 2.5799 & $<\epsilon$ & 0.3142 & 40 & 1127 & 41 & 0 \\
    & DFP & 5.7135 & $<\epsilon$ & 0.1194 & 17 & 475 & 18 & 0 \\
    & BFGS & 1.2967 & $<\epsilon$ & 0.3226 & 44 & 1243 & 45 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{16} &
    DN & 8.2490 & 6.6e-7 & 1.3399 & 100 & 2576 & 100 & 100 \\
    & Cholesky  & 7.4247 & $<\epsilon$ & 0.0789 & 6 & 177 & 7 & 6 \\
    & SR1 & 7.1448 & $<\epsilon$ & 0.1835 & 31 & 881 & 32 & 0 \\
    & DFP & 8.1069 & $<\epsilon$ & 0.2173 & 35 & 995 & 36 & 0 \\
    & BFGS & 7.4247 & $<\epsilon$ & 0.1423 & 24 & 665 & 25 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{32} &
    DN & 5.1104 & 2.4e-7 & 1.9412 & 100 & 2640 & 100 & 100 \\
    & Cholesky & 6.1600 & $<\epsilon$ & 0.1519 & 8 & 231 & 9 & 8 \\
    & SR1 & 6.1600 & $<\epsilon$ & 0.1147 & 19 & 521 & 20 & 0 \\
    & DFP & 2.1522 & $<\epsilon$ & 0.2753 & 49 & 1311 & 50 & 0 \\
    & BFGS & 4.7119 & $<\epsilon$ & 0.1764 & 30 & 823 & 31 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{64} &
    DN & 8.0151 & 1.23e-5 & 3.2566 & 100 & 2524 & 100 & 100 \\
    & Cholesky & 6.7745 & $<\epsilon$ & 0.2441 & 8 & 229 & 9 & 8 \\
    & SR1 & 6.5910 & $<\epsilon$ & 0.0888 & 15 & 417 & 16 & 0 \\
    & DFP & 6.5910 & $<\epsilon$ & 0.1147 & 20 & 533 & 21 & 0 \\
    & BFGS & 6.5910 & $<\epsilon$ & 0.0977 & 15 & 411 & 16 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{128} &
    DN & 6.8139 & 7e-8 & 5.9922 & 100 & 2630 & 100 & 100 \\
    & Cholesky & 5.8812 & $<\epsilon$ & 0.5655 & 10 & 285 & 11 & 10 \\
    & SR1 & 4.5954 & $<\epsilon$ & 0.2074 & 30 & 839 & 31 & 0 \\
    & DFP & 5.7724 & $<\epsilon$ & 0.1311 & 20 & 551 & 21 & 0 \\
    & BFGS & 5.7724 & $<\epsilon$ & 0.1326 & 19 & 525 & 20 & 0 \\
    \bottomrule
  \end{tabular}
  \caption{Newton型方法在Ackley函数上的比较。所有算法均使用Fibonacci法进行精确线搜索。表中DN指阻尼牛顿法（Damped Newton），Cholesky指基于Cholesky分解的修正Newton方法；Time指CPU时间，niter为迭代轮数，feval、geval和Geval分别指Ackley函数、一阶导数及hessian矩阵的计算次数。$\epsilon=1e-8$。}
  \label{tab:overall}
\end{table*}

\begin{itemize}
  \item 阻尼Newton法收敛精度较差，基本无法达到$\epsilon=1e-8$的精度并在完成100次迭代后终止，其余方法均可实现收敛。进一步观察发现，阻尼Newton法所得$||g^*||$虽大于$\epsilon$但也较小，$f^*$也可较好地接近局部极小值，这说明该方法的正确性；如果继续迭代，发现阻尼Newton法会继续“抖动”，$||g^*||$始终无法低于$\epsilon$。为分析其原因，我记录了阻尼Newton的$d_k$并发现其常常并非下降方向，这是因为$G_k$有时并非正定，而遇到上升方向时x就会经历错误的更新进而导致“抖动”。这说明了修正$G_k$的重要性。
  \item Cholesky修正Newton方法通过尝试正定化$G_k$而具有较好的收敛性，相比所有拟Newton方法迭代次数均显著减少，这说明了精确计算$G_k$确实能带来更快的收敛，这展现了Newton方法二次收敛性的意义。
  \item 随着n的增大，虽然Newton方法始终比拟Newton方法迭代次数少，但其CPU时间却逐渐落后于拟Newton方法，这是因为计算Hessian矩阵需要更长的时间（$O(n^2)$），只需计算f和g（$O(n)$）的拟Newton方法在大规模问题上有优势，这说明拟Newton方法对于问题规模有更好地伸缩性。注意，本实验设$g(x)$和$G(x)$都作为函数已知，对于一般数值计算情况，我们需要多个$f(x)$值对这二者进行数值近似，在这种情况下Newton方法相比拟Newton方法也会有更多的函数调用次数。
  \item 拟Newton方法收敛到的函数值普遍小于Newton方法，由于Ackley函数有众多局部极小值，这并不能反映两类方法的收敛效果差别，但我推测这可能是因为拟Newton方法在迭代初期偏向负梯度方向，这使其更快的向$\mathbf{0}$更新，而Newton方法较快地趋向了最近的局部极小值，其函数值相比接近零向量的极小值更大。因此，在Ackley函数这一示例中，拟Newton方法似乎能取得比Newton方法更优的最小化结果，但这一结论并不普适：对于约靠近零向量局部极小值越大的函数，或许我们观察到的结论就会相反。
  \item 在拟Newton方法中，BFGS通常比DFP有更优的数值表现，多次实验也发现BFGS比DFP的收敛更稳定；由于更简单的形式，SR1具有较快的执行速度。
\end{itemize}


\section{进一步分析}
\subsection{不同线搜索算法比较}

\subsection{初始值对收敛性的影响}

\section{总结}



%end{CJK*}
\end{document}
