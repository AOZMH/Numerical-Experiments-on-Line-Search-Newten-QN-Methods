%!TEX program = xelatex

\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[final]{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage[UTF8]{ctex}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{multirow}
\newtheorem{definition}{定义}
\newtheorem{theorem}{定理}
\usepackage{subcaption}
\usepackage[export]{adjustbox}

\setCJKmainfont{SimSun}[AutoFakeBold=2.5,ItalicFont=KaiTi]%
\setCJKsansfont{SimHei}[AutoFakeBold=2.5]%
\setCJKmonofont{FangSong}%


\title{《最优化方法》上机作业2：大规模优化问题的数值实验}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  张旻昊 Minhao Zhang 2101213233 \\
  前沿交叉学科研究院 Academy for Advanced Interdisciplinary Studies\\
  北京大学 Peking University\\
  颐和园路5号，海淀，北京 Yiheyuan Rd. $5^{th}$, Haidian, Beijing\\
  \texttt{minhaozhang@pku.edu.cn} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}
%\begin{CJK*}{UTF8}{gbsn}

\maketitle

\begin{abstract}
  本实验实现了两种共轭梯度方法、基于Beale-Powell重新开始的三项方法及L-BFGS方法的程序，并利用Ackley、Rastrigin等多个函数在大规模设定下测试各方法的收敛表现。
  
\end{abstract}

\section{实验设定}
\subsection{目标函数选取}
本实验使用如下三种目标函数进行测试。在实验中均使用n>100以测试大规模场景下的性能，所有算法均旨在寻找局部极小值。

\subsubsection{Ackley函数}

Ackley函数形式为：
\[ \texttt{Ackley}(x) = -20 exp[-\frac{1}{5} \sqrt{\frac{1}{n}\sum\limits_{i=1}^n x_i^2}] - exp[\frac{1}{n} \sum\limits_{i=1}^n cos(2\pi x_i)] + 20+e \]

Ackley函数具有全局最小值$x^* = (0,0,...,0), f(x^*)=0$，此外它还有众多极小值。

\subsubsection{Griewank函数}
Griewank函数形式为：
\[ \texttt{Griewank}(x) = \frac{1}{4000}\sum\limits_{i=1}^n x_i^2 - \prod\limits_{i=1}^n cos(\frac{x_i}{\sqrt{i}}) + 1 \]
它在$x^* = (0,0,...,0)$处取全局极小值$f(x^*)=0$，同时也具有众多局部极小值。

\subsubsection{Rastrigin函数}
Rastrigin函数形式为：
\[ \texttt{Rastrigin}(x) = 10n + \sum\limits_{i=0}^n [x_i^2 - 10cos(2\pi x_i)] \]
其全局极小值$f(x^*)=0$同样在$x^* = (0,0,...,0)$取得，它也具有众多局部极小值。

\subsection{拟Newton方法}
本实验实现有限内存BFGS方法（L-BFGS），不同于BFGS方法计算$H_k$，它直接计算$d_k$，进而让存储开销从$O(n^2)$降低到$O(n)$。具体地，L-BFGS方法将$H_k$更新公式的最近m步进行展开，这样只需存储最近m步的$s_k = x_k - x_{k-1}$和$yk = g_k - g_{k-1}$，并根据图\ref{fig:lbfgs}即可求得$d_k = -r_{BOUND}$。

\begin{figure}[h]
  \centering
  \includegraphics[width=.8\linewidth]{pics/lbfgs_two_loop.jpg}
  \caption{L-BFGS方法通过two-loop算法更新$d_k$。}
  \label{fig:lbfgs}
\end{figure}

\subsection{共轭梯度法}
本实验实现基于FR与PRP两种$\beta_k$更新公式的共轭梯度方法，具体实现与教材中完全相同，这里不再赘述。

\subsection{三项方法}
共轭梯度法实际上是一种二项的梯度型方法，为提升求解精度，Beale-Powell提出了一种包含三项的方向更新公式，本实验实现了这一方法。此算法基于如下更新公式：
\[ \beta_k = g_k^T [g_k - g_{k-1}] / d_{k-1}^T [g_k - g_{k-1}] \]
\[ \gamma_k = g_k^T [g_{t+1} - g_t] / d_t^T [g_{t+1} - g_t] \]
\[ d_k = -g_k + \beta_k d_{k-1} + \gamma_k d_{t} \]

此外，当更新方向不够好时，使用restart的方法更新t=k-1，并设置$\gamma_k=0$（即使用二项公式更新）。存在如下三种restart条件：1)当$g_k^T g_{k-1} \geq 0.2 ||g_k||^2$时，梯度正交性不足，设置重新开始；2)当$k-t\geq n$时，所有理想共轭方向已经用过了，重新开始；3)求出上述$d_k$后，若$-1.2||g_k||^2\leq d_k^T g_k\leq -0.8||g_k||^2$不满足，则重新开始，并重新舍弃第三项计算$d_k$。


\section{实验结果}
本节分别在三个测试函数上比较各方法的实验效果。在实验中，均使用20次迭代的Fibonacci搜索进行精确线搜索；作为基准方法，我们同时与基于Cholesky分解的修正Newton方法、BFGS拟Newton方法进行比较。

\subsection{Ackley函数上的实验}
表\ref{tab:ackley}总结了上述方法在Ackley函数上的结果。对此可做如下分析：
\begin{itemize}
  \item 对比拟Newton方法和共轭梯度类方法，可以发现前者总体优于后者。所有拟Newton方法在所有规模下均实现收敛，而三类共轭梯度类方法常常无法收敛，达到最大循环次数退出。虽然后者的一阶性具有更高性能，但由于收敛性不佳，往往耗费更多迭代次数，这反而影响了它的效率。
  \item 共轭梯度类方法在正定二次函数中具有很好的理论表现，但在此处实际表现不佳，说明了实际问题的复杂性，我们通常不能期望在一小类问题上具有良好理论基础的方法总是具有很好的泛化能力，实际上，共轭梯度法并不比拟Newton法更通用。
  \item 在拟Newton方法中，L-BFGS相比BFGS具有更优的数值表现，这体现在它具有更快的运行速度。二者虽然总迭代次数相仿（L-BFGS有时反而更多），但舍弃Hessian矩阵的存储使L-BFGS效率更高，总体运行时间显著低于BFGS。
  \item 在共轭梯度类方法中，首先PRP方法显著优于FR方法，这体现在FR方法最终退出时$g^*$往往处在1e-1量级，这通常是不可接受的；作为对比，PRP虽经常无法达到$\epsilon$精度，但也有1e-6的精度，可以接受。在正定二次函数中，PRP与FR等价；但在我们的实际应用中，二者性能差别很大，这说明最优化方法往往需要更多的实践分析。
  \item 其次，Beale-Powell三项方法虽属于对PRP的改进，但在Ackley函数上体现的数值表现差别并不明显，它在多数情况下能取得比PRP更优的精度，但此时需要消耗更多的CPU时间，实际中需权衡时间效率与精度选择两算法。
\end{itemize}

\begin{table*}[h]
  \centering
  \begin{tabular}{c l c c c c c c c}
    \toprule
    \bfseries n & \bfseries Methods & \boldmath{$f(x^*)$} & \boldmath $||g(x^*)||$ & \bfseries Time(s) & \bfseries niter & \bfseries feval & \bfseries geval & \bfseries Geval \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{8} &
    DN & 6.3400 & 1.72e-6 & 0.9913 & 100 & 2250 & 100 & 100 \\
    & Cholesky  & 6.0174 & $<\epsilon$ & 0.0967 & 8 & 221 & 9 & 8 \\
    & SR1 & 2.5799 & $<\epsilon$ & 0.3142 & 40 & 1127 & 41 & 0 \\
    & DFP & 5.7135 & $<\epsilon$ & 0.1194 & 17 & 475 & 18 & 0 \\
    & BFGS & 1.2967 & $<\epsilon$ & 0.3226 & 44 & 1243 & 45 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{16} &
    DN & 8.2490 & 6.6e-7 & 1.3399 & 100 & 2576 & 100 & 100 \\
    & Cholesky  & 7.4247 & $<\epsilon$ & 0.0789 & 6 & 177 & 7 & 6 \\
    & SR1 & 7.1448 & $<\epsilon$ & 0.1835 & 31 & 881 & 32 & 0 \\
    & DFP & 8.1069 & $<\epsilon$ & 0.2173 & 35 & 995 & 36 & 0 \\
    & BFGS & 7.4247 & $<\epsilon$ & 0.1423 & 24 & 665 & 25 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{32} &
    DN & 5.1104 & 2.4e-7 & 1.9412 & 100 & 2640 & 100 & 100 \\
    & Cholesky & 6.1600 & $<\epsilon$ & 0.1519 & 8 & 231 & 9 & 8 \\
    & SR1 & 6.1600 & $<\epsilon$ & 0.1147 & 19 & 521 & 20 & 0 \\
    & DFP & 2.1522 & $<\epsilon$ & 0.2753 & 49 & 1311 & 50 & 0 \\
    & BFGS & 4.7119 & $<\epsilon$ & 0.1764 & 30 & 823 & 31 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{64} &
    DN & 8.0151 & 1.23e-5 & 3.2566 & 100 & 2524 & 100 & 100 \\
    & Cholesky & 6.7745 & $<\epsilon$ & 0.2441 & 8 & 229 & 9 & 8 \\
    & SR1 & 6.5910 & $<\epsilon$ & 0.0888 & 15 & 417 & 16 & 0 \\
    & DFP & 6.5910 & $<\epsilon$ & 0.1147 & 20 & 533 & 21 & 0 \\
    & BFGS & 6.5910 & $<\epsilon$ & 0.0977 & 15 & 411 & 16 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{128} &
    DN & 6.8139 & 7e-8 & 5.9922 & 100 & 2630 & 100 & 100 \\
    & Cholesky & 5.8812 & $<\epsilon$ & 0.5655 & 10 & 285 & 11 & 10 \\
    & SR1 & 4.5954 & $<\epsilon$ & 0.2074 & 30 & 839 & 31 & 0 \\
    & DFP & 5.7724 & $<\epsilon$ & 0.1311 & 20 & 551 & 21 & 0 \\
    & BFGS & 5.7724 & $<\epsilon$ & 0.1326 & 19 & 525 & 20 & 0 \\
    \bottomrule
  \end{tabular}
  \caption{各类方法在Ackley函数上的比较。所有算法均使用Fibonacci法进行精确线搜索。表中BP指基于Beale-Powell restart的三项方法，其余方法见前述介绍；Time指CPU时间，niter为迭代轮数，feval、geval和Geval分别指愿函数、一阶导数及hessian矩阵的计算次数。$\epsilon=1e-8$。}
  \label{tab:ackley}
\end{table*}

\subsection{Griewank函数上的实验}


\section{进一步分析}
\subsection{不同线搜索算法比较}
本节比较前文所述的两种线搜索算法，表\ref{tab:line_search}展示了GLL和Fibonacci两种线搜索在BFGS上的实验效果（其他设定与前文相同）。

\begin{table*}[h]
  \centering
  \begin{tabular}{c l c c c c c c c}
    \toprule
    \bfseries n & \bfseries Methods & \boldmath{$f(x^*)$} & \boldmath $||g(x^*)||$ & \bfseries Time(s) & \bfseries niter & \bfseries feval & \bfseries geval & \bfseries Geval \\
    \cmidrule(lr){1-9}
    \multirow{2}{*}{8} &
    Fibonacci & 4.8841 & $<\epsilon$ & 0.2234 & 25 & 701 & 26 & 0 \\
    & GLL  & 5.8176 & $<\epsilon$ & 0.0499 & 37 & 89 & 38 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{2}{*}{16} &
    Fibonacci & 4.5308 & $<\epsilon$ & 0.2781 & 37 & 1015 & 38 & 0 \\
    & GLL  & 2.5799 & $<\epsilon$ & 0.0838 & 68 & 160 & 69 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{2}{*}{32} &
    Fibonacci & 4.6046 & $<\epsilon$ & 0.1855 & 32 & 881 & 33 & 0 \\
    & GLL  & 2.7287 & $<\epsilon$ & 0.0668 & 70 & 150 & 71 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{2}{*}{64} &
    Fibonacci & 6.0415 & $<\epsilon$ & 0.1097 & 19 & 523 & 20 & 0 \\
    & GLL  & 5.4676 & $<\epsilon$ & 0.0379 & 39 & 90 & 40 & 0 \\
    \cmidrule(lr){1-9}
    \multirow{2}{*}{128} &
    Fibonacci & 6.8141 & $<\epsilon$ & 0.1310 & 19 & 517 & 20 & 0 \\
    & GLL  & 6.8680 & $<\epsilon$ & 0.0399 & 23 & 47 & 24 & 0 \\
    \bottomrule
  \end{tabular}
  \caption{GLL与Fibonacci两种线搜索方法在BFGS上的比较。$\epsilon=1e-8$。}
  \label{tab:line_search}
\end{table*}

可见，两算法均可实现收敛（$||g^*||<\epsilon$）；在迭代次数方面，精确线搜索的Fibonacci方法更少，这是因为精确的搜索使每步的步长选取更有效；而在CPU时间、函数调用次数等方面，非精确线搜索GLL方法显著优于精确线搜索，这是因为其每步无需全部完成Fibonacci法的20次函数调用，只需通过指数降低$\alpha$满足GLL准则，从表中可见平均每次搜索仅需1$\sim$2次函数调用，而显著减少的函数调用提高了其运行效率，让它即便需要更多的迭代轮数仍保持更低的耗时。这一结果体现了非精确线搜索的典型特征：通常迭代轮数更多，但函数调用次数显著减少，从而提升算法性能。

\subsection{初始值对收敛性的影响}
如图\ref{fig:ackley}所示，Ackley函数具有众多极小值，$\mathbf{0}$处为其全局最小值，其余的极小值离$\mathbf{0}$越远值越大。因此，初始点离$\mathbf{0}$越远往往算法就越难以收敛到更小的极小值，本节尝试验证这一点。具体地，我们在先随机初始化一个满足标准正态分布的初始向量，再将其依次乘以$k\in \{1,2,4,8,16\}$作为五个初始迭代点，分别运行基于GLL准则线搜索的Cholesky修正Newton方法，结果如表\ref{tab:init_value}所示。

可见，算法在不同初始值上均可收敛，表明其具有较好的全局收敛性。对于所有问题规模n，最终收敛到的函数值$f^*$随k的增大而增大，这符合上述结论；此外，k的变化对算法执行时间、函数调用次数等指标的影响并不显著，这说明在不同初始值处寻找局部极小值的难度差别并不十分显著。

这些结果也对我们进行数值计算实践给出了一点启发：通常而言，我们希望得到尽可能小的目标函数值，亦即寻找一个相对更优的局部极小值，在这一设定下初始值的选取变得重要，往往取得离最优局部极小值更近的初始值（例如这里的k=1）可以帮助算法收敛到更小的极值点。这一点结论与当下深度学习领域对预训练、规范初始化的相关工作之动机与结论相契合。

\begin{table*}[h]
  \centering
  \begin{tabular}{c c c c c c c c c}
    \toprule
    \bfseries n & \bfseries k & \boldmath{$f(x^*)$} & \boldmath $||g(x^*)||$ & \bfseries Time(s) & \bfseries niter & \bfseries feval & \bfseries geval & \bfseries Geval \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{8} &
       1 &  2.5799 & $<\epsilon$ & 0.0400 &  8 & 27 &  9 & 8 \\
    &  2 &  5.4958 & $<\epsilon$ & 0.0563 & 12 & 37 & 13 & 12 \\
    &  4 & 11.7645 & $<\epsilon$ & 0.0458 & 10 & 31 & 11 & 10 \\
    &  8 & 13.6390 & $<\epsilon$ & 0.0370 &  8 & 29 &  9 & 8 \\
    & 16 & 19.0864 & $<\epsilon$ & 0.0242 &  5 & 16 &  6 & 5 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{16} &
       1 &  2.5799 & $<\epsilon$ & 0.1539 & 20 & 61 & 21 & 20 \\
    &  2 &  7.6225 & $<\epsilon$ & 0.1101 & 15 & 46 & 16 & 15 \\
    &  4 & 11.9124 & $<\epsilon$ & 0.0983 &  9 & 29 & 10 & 9 \\
    &  8 & 18.0296 & $<\epsilon$ & 0.0984 & 13 & 40 & 14 & 13 \\
    & 16 & 18.2010 & $<\epsilon$ & 0.1128 & 11 & 34 & 12 & 11 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{32} &
       1 &  3.4689 & $<\epsilon$ & 0.2239 & 17 & 61 & 18 & 17 \\
    &  2 &  6.9592 & $<\epsilon$ & 0.1069 &  8 & 29 &  9 & 8 \\
    &  4 & 11.7761 & $<\epsilon$ & 0.1337 & 10 & 41 & 11 & 10 \\
    &  8 & 15.1418 & $<\epsilon$ & 0.1200 &  9 & 31 & 10 & 9 \\
    & 16 & 19.3774 & $<\epsilon$ & 0.2293 & 15 & 46 & 16 & 15 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{64} &
       1 &  3.6751 & $<\epsilon$ & 0.4887 & 20 & 68 & 21 & 20 \\
    &  2 &  7.1266 & $<\epsilon$ & 0.4013 & 16 & 49 & 17 & 16 \\
    &  4 & 12.1688 & $<\epsilon$ & 0.4218 & 17 & 86 & 18 & 17 \\
    &  8 & 16.6058 & $<\epsilon$ & 0.1751 &  7 & 24 &  8 & 7 \\
    & 16 & 19.1020 & $<\epsilon$ & 0.2447 & 10 & 34 & 11 & 10 \\
    \cmidrule(lr){1-9}
    \multirow{5}{*}{128} &
       1 &  5.2257 & $<\epsilon$ & 0.6786 & 14 & 70 & 15 & 14 \\
    &  2 &  6.5065 & $<\epsilon$ & 0.3273 &  7 & 28 &  8 & 7 \\
    &  4 & 12.6906 & $<\epsilon$ & 0.3303 &  7 & 25 &  8 & 7 \\
    &  8 & 17.0561 & $<\epsilon$ & 0.3845 &  8 & 29 &  9 & 8 \\
    & 16 & 19.3248 & $<\epsilon$ & 0.8478 & 18 & 60 & 19 & 18 \\
    \bottomrule
  \end{tabular}
  \caption{不同初始值选取对Cholesky修正Newton方法的影响。其中k代表初始点相较标准正态分布的倍数，$\epsilon=1e-8$。}
  \label{tab:init_value}
\end{table*}


\section{总结}
本文比较了多种Newton和拟Newton方法在Ackley函数上的数值表现，分析了二者的优劣；本文还针对不同线搜索策略、不同初始迭代值对收敛过程的影响进行了数值实验及分析，为未来在不同问题上应用各类技巧给出了一些经验性分析。


%end{CJK*}
\end{document}
